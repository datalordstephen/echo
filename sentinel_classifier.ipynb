{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# UrbanSound8K Audio Classifier (Sentinel)\n",
                "\n",
                "This notebook implements a multi-class audio classifier using the UrbanSound8K dataset. \n",
                "It uses a ResNet18 CNN model and evaluates performance using 10-fold cross-validation.\n",
                "\n",
                "**Requirements**:\n",
                "- Google Drive must contain the `UrbanSound8K` folder at `My Drive/UrbanSound8K`.\n",
                "- GPU Runtime is recommended."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import torchaudio\n",
                "import torchaudio.transforms as T\n",
                "from torchvision.models import resnet18\n",
                "from sklearn.metrics import accuracy_score, f1_score\n",
                "from tqdm.notebook import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import librosa\n",
                "import librosa.display\n",
                "from sklearn.utils.class_weight import compute_class_weight\n",
                "\n",
                "print(\"PyTorch Version:\", torch.__version__)\n",
                "print(\"TorchAudio Version:\", torchaudio.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration & Constants\n",
                "DATA_ROOT = '/content/drive/My Drive/UrbanSound8K'\n",
                "AUDIO_DIR = os.path.join(DATA_ROOT, 'audio')\n",
                "METADATA_PATH = os.path.join(DATA_ROOT, 'metadata', 'UrbanSound8K.csv')\n",
                "\n",
                "SAMPLE_RATE = 22050\n",
                "DURATION = 4 # seconds\n",
                "TARGET_SAMPLE_COUNT = SAMPLE_RATE * DURATION\n",
                "\n",
                "N_MELS = 128\n",
                "HOP_LENGTH = 512\n",
                "N_FFT = 2048\n",
                "\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 10 \n",
                "LEARNING_RATE = 0.001\n",
                "NUM_CLASSES = 5\n",
                "FOLDS = 10\n",
                "\n",
                "# Class Mapping\n",
                "# 0: Siren (8)\n",
                "# 1: Jackhammer (7)\n",
                "# 2: Gunshot (6)\n",
                "# 3: Street Music (9)\n",
                "# 4: Background (0, 1, 2, 3, 4, 5)\n",
                "CLASS_MAP = {\n",
                "    8: 0, 7: 1, 6: 2, 9: 3,\n",
                "    0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4\n",
                "}\n",
                "CLASS_NAMES = ['Siren', 'Jackhammer', 'Gunshot', 'Street Music', 'Background']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper to apply mapping\n",
                "def map_classes(df, mapping):\n",
                "    df['new_label'] = df['classID'].map(mapping)\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data Exploration: Load Metadata & Visualize Distribution\n",
                "if os.path.exists(METADATA_PATH):\n",
                "    metadata = pd.read_csv(METADATA_PATH)\n",
                "    print(\"Original Metadata Info:\")\n",
                "    print(metadata.head())\n",
                "    \n",
                "    # Apply Mapping\n",
                "    metadata = map_classes(metadata, CLASS_MAP)\n",
                "    print(\"\\nMetadata after mapping:\")\n",
                "    print(metadata[['slice_file_name', 'fold', 'classID', 'class', 'new_label']].head())\n",
                "    \n",
                "    # Visualize Class Distribution\n",
                "    plt.figure(figsize=(10, 5))\n",
                "    metadata['new_label'].value_counts().sort_index().plot(kind='bar')\n",
                "    plt.title('Class Distribution (Mapped)')\n",
                "    plt.xticks(ticks=range(5), labels=CLASS_NAMES, rotation=45)\n",
                "    plt.xlabel('Class')\n",
                "    plt.ylabel('Count')\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"WARNING: Metadata file not found. Ensure Drive is mounted.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset Class\n",
                "class UrbanSoundDataset(Dataset):\n",
                "    def __init__(self, df, audio_dir, transformation, target_sample_rate, num_samples):\n",
                "        self.df = df\n",
                "        self.audio_dir = audio_dir\n",
                "        self.transformation = transformation\n",
                "        self.target_sample_rate = target_sample_rate\n",
                "        self.num_samples = num_samples\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, index):\n",
                "        row = self.df.iloc[index]\n",
                "        # Construct path: audio/foldX/filename\n",
                "        audio_path = os.path.join(self.audio_dir, f\"fold{row['fold']}\", row['slice_file_name'])\n",
                "        # Use the mapped label\n",
                "        label = row['new_label']\n",
                "\n",
                "        try:\n",
                "            # Load audio\n",
                "            signal, sr = torchaudio.load(audio_path)\n",
                "            \n",
                "            # Resample if needed\n",
                "            if sr != self.target_sample_rate:\n",
                "                resampler = T.Resample(sr, self.target_sample_rate)\n",
                "                signal = resampler(signal)\n",
                "            \n",
                "            # Mix down to mono if stereo\n",
                "            if signal.shape[0] > 1:\n",
                "                signal = torch.mean(signal, dim=0, keepdim=True)\n",
                "            \n",
                "            # Pad or Truncate\n",
                "            length_signal = signal.shape[1]\n",
                "            if length_signal > self.num_samples:\n",
                "                signal = signal[:, :self.num_samples]\n",
                "            elif length_signal < self.num_samples:\n",
                "                num_missing = self.num_samples - length_signal\n",
                "                signal = torch.nn.functional.pad(signal, (0, num_missing))\n",
                "            \n",
                "            # Apply MelSpectrogram\n",
                "            # Note: We return both signal and transformed signal for visualization if needed, \n",
                "            # but for training we usually just need the transformed one.\n",
                "            # Here we apply transform immediately.\n",
                "            spec = self.transformation(signal)\n",
                "            \n",
                "            return spec, label\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Error loading {audio_path}: {e}\")\n",
                "            # Return a dummy tensor\n",
                "            dummy_signal = torch.zeros((1, N_MELS, (self.num_samples // HOP_LENGTH) + 1))\n",
                "            return dummy_signal, label"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Audio Transforms\n",
                "mel_spectrogram = T.MelSpectrogram(\n",
                "    sample_rate=SAMPLE_RATE,\n",
                "    n_fft=N_FFT,\n",
                "    hop_length=HOP_LENGTH,\n",
                "    n_mels=N_MELS\n",
                ")\n",
                "\n",
                "class LogMelSpectrogram(nn.Module):\n",
                "    def __init__(self, mel_spectrogram):\n",
                "        super().__init__()\n",
                "        self.mel_spectrogram = mel_spectrogram\n",
                "        self.amplitude_to_db = T.AmplitudeToDB()\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.mel_spectrogram(x)\n",
                "        x = self.amplitude_to_db(x)\n",
                "        return x\n",
                "\n",
                "audio_transform = LogMelSpectrogram(mel_spectrogram)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Check Spectrograms\n",
                "if os.path.exists(METADATA_PATH):\n",
                "    # Create a dummy dataset for visualization\n",
                "    # We pick one sample from each class\n",
                "    print(\"Loading samples for visualization...\")\n",
                "    vis_df = metadata.groupby('new_label').apply(lambda x: x.iloc[0]).reset_index(drop=True)\n",
                "    vis_ds = UrbanSoundDataset(vis_df, AUDIO_DIR, audio_transform, SAMPLE_RATE, TARGET_SAMPLE_COUNT)\n",
                "    \n",
                "    plt.figure(figsize=(15, 10))\n",
                "\n",
                "    for i in range(len(vis_ds)):\n",
                "        spec, label = vis_ds[i]\n",
                "        \n",
                "        plt.subplot(2, 3, i+1)\n",
                "        plt.imshow(spec.squeeze().numpy(), aspect='auto', origin='lower')\n",
                "        plt.title(f\"Class: {CLASS_NAMES[label]}\")\n",
                "        plt.axis('off')\n",
                "    plt.show()\n",
                "    print(\"Spectrogram shape:\", spec.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model\n",
                "class AudioClassifier(nn.Module):\n",
                "    def __init__(self, num_classes):\n",
                "        super().__init__()\n",
                "        self.model = resnet18(pretrained=True)\n",
                "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
                "        num_ftrs = self.model.fc.in_features\n",
                "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.model(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training Functions\n",
                "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for inputs, labels in tqdm(dataloader, desc='Training', leave=False):\n",
                "        inputs, labels = inputs.to(device), labels.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(inputs)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        running_loss += loss.item()\n",
                "        _, predicted = outputs.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += predicted.eq(labels).sum().item()\n",
                "        \n",
                "    return running_loss / len(dataloader), correct / total\n",
                "\n",
                "def validate_epoch(model, dataloader, criterion, device):\n",
                "    model.eval()\n",
                "    running_loss = 0.0\n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for inputs, labels in dataloader:\n",
                "            inputs, labels = inputs.to(device), labels.to(device)\n",
                "            outputs = model(inputs)\n",
                "            loss = criterion(outputs, labels)\n",
                "            \n",
                "            running_loss += loss.item()\n",
                "            _, predicted = outputs.max(1)\n",
                "            all_preds.extend(predicted.cpu().numpy())\n",
                "            all_labels.extend(labels.cpu().numpy())\n",
                "            \n",
                "    accuracy = accuracy_score(all_labels, all_preds)\n",
                "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
                "    return running_loss / len(dataloader), accuracy, f1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Main Loop\n",
                "if not os.path.exists(METADATA_PATH):\n",
                "    print(f\"ERROR: Metadata file not found at {METADATA_PATH}\")\n",
                "else:\n",
                "    # Note: Metadata loaded above in exploration step, but strictly ensuring it exists here for safe execution\n",
                "    # if user skipped cells.\n",
                "    metadata = pd.read_csv(METADATA_PATH)\n",
                "    metadata = map_classes(metadata, CLASS_MAP)\n",
                "\n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "    print(f\"Device: {device}\")\n",
                "\n",
                "    fold_accuracies = []\n",
                "    fold_f1s = []\n",
                "\n",
                "    for fold in range(1, FOLDS + 1):\n",
                "        print(f\"\\n{'='*20} Fold {fold}/{FOLDS} {'='*20}\")\n",
                "        \n",
                "        train_df = metadata[metadata['fold'] != fold]\n",
                "        val_df = metadata[metadata['fold'] == fold]\n",
                "        \n",
                "        # Calculate Weights for Imbalance\n",
                "        class_counts = train_df['new_label'].value_counts().sort_index().values\n",
                "        class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_df['new_label']), y=train_df['new_label'])\n",
                "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
                "\n",
                "        train_ds = UrbanSoundDataset(train_df, AUDIO_DIR, audio_transform, SAMPLE_RATE, TARGET_SAMPLE_COUNT)\n",
                "        val_ds = UrbanSoundDataset(val_df, AUDIO_DIR, audio_transform, SAMPLE_RATE, TARGET_SAMPLE_COUNT)\n",
                "        \n",
                "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
                "        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
                "        \n",
                "        model = AudioClassifier(NUM_CLASSES).to(device)\n",
                "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "        \n",
                "        # Weighted Cross Entropy\n",
                "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
                "        \n",
                "        best_val_acc = 0.0\n",
                "        best_val_f1 = 0.0\n",
                "        \n",
                "        for epoch in range(EPOCHS):\n",
                "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
                "            val_loss, val_acc, val_f1 = validate_epoch(model, val_loader, criterion, device)\n",
                "            \n",
                "            print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} F1: {val_f1:.4f}\")\n",
                "            \n",
                "            if val_acc > best_val_acc:\n",
                "                best_val_acc = val_acc\n",
                "                best_val_f1 = val_f1\n",
                "        \n",
                "        fold_accuracies.append(best_val_acc)\n",
                "        fold_f1s.append(best_val_f1)\n",
                "        print(f\"--> Best Validation Accuracy for Fold {fold}: {best_val_acc:.4f}\")\n",
                "\n",
                "    print(\"\\n\" + \"*\"*30)\n",
                "    print(\"FINAL RESULTS ACROSS 10 FOLDS\")\n",
                "    print(\"*\"*30)\n",
                "    print(f\"Average Accuracy: {np.mean(fold_accuracies):.4f}\")\n",
                "    print(f\"Average F1 Score: {np.mean(fold_f1s):.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}